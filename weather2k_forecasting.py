# -*- coding: utf-8 -*-
"""Weather2k Forecasting

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lwiZxcui8P1evXC7L16AlrkbDoMdiMu-
"""



import os
import numpy as np
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.nn as nn

import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, DistributedSampler
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler



class MinMax01Scaler:
    """
    Standard the input
    """

    def __init__(self, min, max):
        self.min = min
        self.max = max

    def transform(self, data):
        return (data - self.min) / (self.max - self.min)

    def inverse_transform(self, data):
        if type(data) == torch.Tensor and type(self.min) == np.ndarray:
            self.min = torch.from_numpy(self.min).to(data.device).type(data.dtype)
            self.max = torch.from_numpy(self.max).to(data.device).type(data.dtype)
        return (data * (self.max - self.min) + self.min)

def normalize_dataset(data, normalizer, column_wise=False):
    # data : t, n, 5
    if normalizer == 'max01':
        if column_wise:
            minimum = data.min(axis=(0, 1), keepdims=True)
            maximum = data.max(axis=(0, 1), keepdims=True)
        else:
            minimum = data.min()
            maximum = data.max()
        scaler = MinMax01Scaler(minimum, maximum)
        data = scaler.transform(data)
        print('Normalize the dataset by MinMax01 Normalization')

    return data, scaler


def split_data_by_ratio(data, val_ratio, test_ratio, shffule = False):
    data_len = len(data)
    test_data = data[-int(data_len * test_ratio):]
    if shffule:
        tra_val_data = data[:-int(data_len * test_ratio)]
        shuffle_idx = np.random.permutation(len(tra_val_data))
        tra_val_data = tra_val_data[shuffle_idx]
        val_data = tra_val_data[-int(data_len * val_ratio):]
        train_data = tra_val_data[:-int(data_len * val_ratio)]
    else:
        val_data = data[-int(data_len * (test_ratio + val_ratio)):-int(data_len * test_ratio)]
        train_data = data[:-int(data_len * (test_ratio + val_ratio))]
    return train_data, val_data, test_data



def Add_Window_Horizon(data, window=3, horizon=1, single=False):
    '''
    :param data: shape [B, ...]
    :param window:
    :param horizon:
    :return: X is [B, W, ...], Y is [B, H, ...]
    '''
    length = len(data)
    end_index = length - horizon - window + 1
    X = []  # windows
    Y = []  # horizon
    index = 0
    if single:
        while index < end_index:
            X.append(data[index:index + window])
            Y.append(data[index + window + horizon - 1:index + window + horizon])
            index = index + 1
    else:
        while index < end_index:
            X.append(data[index:index + window])
            Y.append(data[index + window:index + window + horizon])
            index = index + 1
    # X = np.array(X)
    # Y = np.array(Y)
    X = np.stack(X, axis=0)
    Y = np.stack(Y, axis=0)
    return X, Y



import copy

def is_sym(adj):
    return np.allclose(adj, adj.T)

def pair_dist_to_adj(adj_dist, num_of_vertices,
                         type_='connectivity', self_loop = False, args = None):
    '''
    Parameters
    ----------
    adj_dist: (N x N)
    num_of_vertices: int, the number of vertices
    type_: str, {connectivity, distance}
    Returns
    ----------
    A: np.ndarray, adjacency matrix
    '''
    adj_th = 0.1
    sigma = 10
    A = copy.deepcopy(adj_dist)
    if type_ in ['connectivity', 'distance_2']:
        trans_A = np.exp(- A ** 2 / (sigma ** 2))
    elif type_ == 'distance':
        trans_A = A
    else:
        raise ValueError("type_ error, must be "
                                 "connectivity or distance!")


    # Fills cells in the matrix with distances.
    if type_ == 'connectivity':
        trans_A[trans_A >= adj_th] = 1
        trans_A[trans_A < adj_th] = 0
    elif type_ == 'distance_2':
        trans_A[trans_A < adj_th] = 0
    elif type_ == 'distance':
        non_zero_indices = np.nonzero(trans_A)
        trans_A[non_zero_indices] = 1 / trans_A[non_zero_indices]
        trans_A[trans_A == 0] = 1
    else:
        raise ValueError("type_ error, must be "
                                 "connectivity or distance!")
    print('{} Adj edge num: {}'.format(type_, np.count_nonzero(trans_A)))
    assert is_sym(trans_A), 'the adj is not symmetric.'
    if self_loop is True:
        np.fill_diagonal(trans_A, 1)

    return trans_A

def build_adj(adj_dist):
    num_nodes = 1866
    adj_ori = pair_dist_to_adj(adj_dist, num_nodes, type_='connectivity', self_loop = True)
    adj_weight = pair_dist_to_adj(adj_dist, num_nodes, type_='distance_2', self_loop = True)

    return adj_ori, adj_weight



def data_loader(X, Y, batch_size, shuffle=True, drop_last=True):
    cuda = True if torch.cuda.is_available() else False
    print('cuda :{}'.format(cuda))
    TensorFloat = torch.cuda.FloatTensor if cuda else torch.FloatTensor
    X, Y = TensorFloat(X), TensorFloat(Y)

    data = torch.utils.data.TensorDataset(X, Y)
    sampler = DistributedSampler(data, shuffle=shuffle, drop_last=drop_last)
    per_gpu_batch = max(1, batch_size // dist.get_world_size())
    dataloader = torch.utils.data.DataLoader(data, batch_size=per_gpu_batch, sampler=sampler)
    return dataloader



"""# Graph Attention Network

## Graph Attention Layers
"""

import torch
from torch import nn
import torch.nn.functional as F


class GraphAttentionLayer(nn.Module):
    """
    Graph Attention Layer (GAT) as described in the paper `"Graph Attention Networks" <https://arxiv.org/pdf/1710.10903.pdf>`.

        This operation can be mathematically described as:

            e_ij = a(W h_i, W h_j)
            α_ij = softmax_j(e_ij) = exp(e_ij) / Σ_k(exp(e_ik))
            h_i' = σ(Σ_j(α_ij W h_j))

            where h_i and h_j are the feature vectors of nodes i and j respectively, W is a learnable weight matrix,
            a is an attention mechanism that computes the attention coefficients e_ij, and σ is an activation function.

    """
    def __init__(self, in_features: int, out_features: int, n_heads: int, concat: bool = False, dropout: float = 0.4, leaky_relu_slope: float = 0.2):
        super().__init__()

        self.n_heads = n_heads # Number of attention heads
        self.concat = concat # wether to concatenate the final attention heads
        self.dropout = dropout # Dropout rate

        if concat: # concatenating the attention heads
            self.out_features = out_features # Number of output features per node
            assert out_features % n_heads == 0 # Ensure that out_features is a multiple of n_heads
            self.n_hidden = out_features // n_heads
        else: # averaging output over the attention heads (Used in the main paper)
            self.n_hidden = out_features

        #  A shared linear transformation, parametrized by a weight matrix W is applied to every node
        #  Initialize the weight matrix W
        self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * n_heads)))

        # Initialize the attention weights a
        self.a = nn.Parameter(torch.empty(size=(n_heads, 2 * self.n_hidden, 1)))

        self.leakyrelu = nn.LeakyReLU(leaky_relu_slope) # LeakyReLU activation function
        self.softmax = nn.Softmax(dim=1) # softmax activation function to the attention coefficients

        self.reset_parameters() # Reset the parameters
        # self.interaction_matrix_u = nn.Parameter(torch.ones(n_heads, 1866, 100))
        # self.interaction_matrix_v = nn.Parameter(torch.ones(n_heads, 100, 1866))
        # nn.init.xavier_uniform_(self.interaction_matrix_u.data, gain=1.414)
        # nn.init.xavier_uniform_(self.interaction_matrix_v.data, gain=1.414)
        self.interaction_matrix = nn.Parameter(torch.ones(n_heads, 1866, 1866))
        nn.init.xavier_uniform_(self.interaction_matrix.data, gain=1.414)




        # nn.init.xavier_uniform_(self.interaction_matrix.data, gain=1.414)

    def reset_parameters(self):
        """
        Reinitialize learnable parameters.
        """
        nn.init.xavier_normal_(self.W)
        nn.init.xavier_normal_(self.a)


    def _get_attention_scores(self, h_transformed: torch.Tensor):
        """calculates the attention scores e_ij for all pairs of nodes (i, j) in the graph
        in vectorized parallel form. for each pair of source and target nodes (i, j),
        the attention score e_ij is computed as follows:

            e_ij = LeakyReLU(a^T [Wh_i || Wh_j])

            where || denotes the concatenation operation, and a and W are the learnable parameters.

        Args:
            h_transformed (torch.Tensor): Transformed feature matrix with shape (n_nodes, n_heads, n_hidden),
                where n_nodes is the number of nodes and out_features is the number of output features per node.

        Returns:
            torch.Tensor: Attention score matrix with shape (n_heads, n_nodes, n_nodes), where n_nodes is the number of nodes.
        """

        source_scores = torch.matmul(h_transformed, self.a[:, :self.n_hidden, :])
        target_scores = torch.matmul(h_transformed, self.a[:, self.n_hidden:, :])

        # broadcast add
        # (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)
        e = source_scores + target_scores.mT
        return self.leakyrelu(e)

    def forward(self,  h: torch.Tensor, adj_mat: torch.Tensor):
        """
        Performs a graph attention layer operation.

        Args:
            h (torch.Tensor): Input tensor representing node features.
            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.

        Returns:
            torch.Tensor: Output tensor after the graph convolution operation.
        """
        B, N, Q = h.shape

        # Apply linear transformation to node feature -> W h
        # output shape (n_nodes, n_hidden * n_heads)

        h_transformed = torch.matmul(h, self.W)  # (B, N, n_heads*n_hidden)
        # h_transformed = F.dropout(h_transformed, p=self.dropout, training=self.training)
        h_transformed = h_transformed.view(B, N, self.n_heads, self.n_hidden).permute(0, 2, 1, 3)  # (B, H, N, d_h)

        # getting the attention scores
        # output shape (n_heads, n_nodes, n_nodes)
        # e = self._get_attention_scores(h_transformed)

        # # e: (H, N, N)
        # e = self.interaction_matrix_u @ self.interaction_matrix_v
        e = self.interaction_matrix
        e = (e + e.transpose(-1, -2)) / 2
        e = e / (torch.linalg.norm(e, dim=-1, keepdim=True) + 1e-8)
        # e = torch.nn.functional.layer_norm(e, e.shape[-1:])
        e = torch.nn.functional.layer_norm(e, normalized_shape=(e.shape[-2], e.shape[-1]))



        # Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)
        # connectivity_mask = -9e16 * torch.ones_like(e)
        # e = torch.where(adj_mat > 0, e, connectivity_mask) # masked attention scores

        # attention coefficients are computed as a softmax over the rows
        # for each column j in the attention score matrix e
        attention = F.softmax(e, dim=-1)
        # attention = F.dropout(attention, self.dropout, training=self.training)

        # final node embeddings are computed as a weighted average of the features of its neighbors
        h_prime = torch.matmul(attention, h_transformed)

        # concatenating/averaging the attention heads
        # output shape (n_nodes, out_features)
        if self.concat:
            h_prime = h_prime.permute(0, 2, 1, 3).contiguous().view(B, N, self.out_features)  # (B, N, out_features)
            # h_prime = h_prime.permute(1, 0, 2).contiguous().view(n_nodes, self.out_features)
        else:
            # h_prime = h_prime.mean(dim=0)
            h_prime = h_prime.mean(dim=1)                                        # (B, N, d_h)


        return h_prime

import torch
from torch import nn
import torch.nn.functional as F

################################
###    GAT NETWORK MODULE    ###
################################

class GAT(nn.Module):
    """
    Graph Attention Network (GAT) as described in the paper `"Graph Attention Networks" <https://arxiv.org/pdf/1710.10903.pdf>`.
    Consists of a 2-layer stack of Graph Attention Layers (GATs). The fist GAT Layer is followed by an ELU activation.
    And the second (final) layer is a GAT layer with a single attention head and softmax activation function.
    """
    def __init__(self,
        in_features,
        n_hidden,
        n_heads,
        concat=False,
        dropout=0.2,
        leaky_relu_slope=0.2):
        """ Initializes the GAT model.

        Args:
            in_features (int): number of input features per node.
            n_hidden (int): output size of the first Graph Attention Layer.
            n_heads (int): number of attention heads in the first Graph Attention Layer.
            num_classes (int): number of classes to predict for each node.
            concat (bool, optional): Wether to concatinate attention heads or take an average over them for the
                output of the first Graph Attention Layer. Defaults to False.
            dropout (float, optional): dropout rate. Defaults to 0.4.
            leaky_relu_slope (float, optional): alpha (slope) of the leaky relu activation. Defaults to 0.2.
        """

        super(GAT, self).__init__()

        # Define the Graph Attention layers
        self.gat1 = GraphAttentionLayer(
            in_features=in_features, out_features=n_hidden, n_heads=n_heads,
            concat=concat, dropout=dropout, leaky_relu_slope=leaky_relu_slope
            )

        self.gat2 = GraphAttentionLayer(
            in_features=n_hidden, out_features=n_hidden, n_heads=1,
            concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope
            )


    def forward(self, input_tensor: torch.Tensor , adj_mat: torch.Tensor):
        """
        Performs a forward pass through the network.

        Args:
            input_tensor (torch.Tensor): Input tensor representing node features.
            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.

        Returns:
            torch.Tensor: Output tensor after the forward pass.
        """

        # Apply the first Graph Attention layer
        x = self.gat1(input_tensor, adj_mat)
        x = F.elu(x) # Apply ELU activation function to the output of the first layer

        # Apply the second Graph Attention layer
        x = self.gat2(x, adj_mat)

        return x # Apply log softmax activation function

"""## GAT GRU Forecast"""

import torch
from torch import nn
from typing import Optional
from torch.func import vmap


class GAT_GRU_Forecaster(nn.Module):
    """
    x:  (B, T, N, F_in)
    adj:(N, N)
    Returns: (B, N, output_dim)
    """
    def __init__(self,
                 n_feat,
                 n_hidden,
                 n_heads,
                 output_dim,
                 gru_hidden,
                 num_layers: int = 1,
                 dropout: float = 0.0,
                 use_vmap: bool = True):
        super().__init__()

        self.gat = GAT(n_feat, n_hidden, n_heads)
        self.gru_hidden = gru_hidden
        self.use_vmap = use_vmap
        self.n_hidden = n_hidden

        self.norm = nn.LayerNorm(n_hidden)  # optional but stabilizes across timesteps
        self.gru = nn.GRU(
            input_size=n_hidden,
            hidden_size=self.gru_hidden,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
            bidirectional=False
        )
        self.head = nn.Linear(self.gru_hidden, output_dim)

    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:
        B, T, N, Fin = x.shape
        device = x.device
        # Ensure adj on same device/dtype
        adj = adj.to(device=device, dtype=x.dtype)

        # ---- 1) Run GAT at each time step ----
        # x_bt: (B*T, N, Fin)
        x_bt = x.view(B * T, N, Fin)

        z_bt = self.gat(x_bt, adj)

        z_bt = self.norm(z_bt)

        # ---- 2) GRU per node over time ----
        # reshape to sequences of length T for each node
        z = z_bt.view(B, T, N, self.n_hidden)              # (B, T, N, H)
        z_seq = z.permute(0, 2, 1, 3).contiguous()          # (B, N, T, H)
        z_seq = z_seq.view(B * N, T, self.n_hidden)        # (B*N, T, H)

        y, _ = self.gru(z_seq)                              # (B*N, T, H_gru)
        y_last = y[:, -1]                                   # last step per sequence: (B*N, H_gru)

        # ---- 3) Map to output per node ----
        out = self.head(y_last).view(B, N, -1)              # (B, N, output_dim)

        return out

"""## HyperParams"""



class Hyperparameters:
    # Hyperparameters

    def __init__(self, tra_loader, adj):
        self.K = 70
        self.learning_rate = 3e-4
        self.epochs = 10
        self.batch_size = 8
        self.n_feat = 1  # Number of features for each node (adjust according to your dataset)
        self.n_hidden = 32
        self.gru_hidden = 32
        self.n_heads = 4
        self.dropout = 0.6
        self.alpha = 0.2
        self.output_dim = 12  # Traffic flow prediction output (1 for single step, adjust as needed)
        self.seq_len = 12  # Sequence length (time steps)
        self.n_nodes = 1866  # Number of nodes (traffic sensors, locations, etc.)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.gamma=0.9 # Covariance update factor
        # train_loader = data_module.train_dataloader()
        self.train_loader = tra_loader
        self.weight_decay=1e-4
        # Loss function
        self.criterion = nn.MSELoss()
        # adj = data_module._adj
        self.adj_mat = torch.from_numpy(adj).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

        self.checkpoint_dir = 'checkpoints'
        self.best_model_dir = 'best_model'
        self.meta_data_dir = 'meta_data'
        self.interaction_matrix_dir = 'interaction_matrix'
        self.interaction_matrix_norm_dir = 'interaction_matrix_norm'
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(self.best_model_dir, exist_ok=True)
        os.makedirs(self.meta_data_dir, exist_ok=True)
        os.makedirs(self.interaction_matrix_dir, exist_ok=True)
        os.makedirs(self.interaction_matrix_norm_dir, exist_ok=True)

        self.best_val_loss = float('inf')
        self.best_model_path = None



def reduce_mean(t):
    dist.all_reduce(t, op=dist.ReduceOp.SUM)
    t /= dist.get_world_size()
    return t

def train(rank, world_size, model, optimizer, hyperparameters):
    """Training function for each GPU process.

    Args:
        rank (int): The rank of the current process (one per GPU).
        world_size (int): Total number of processes.
    """
    # Initialize the process group for distributed training
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    device = torch.device(f"cuda:{rank}")  # Set device to current GPU

    # Instantiate the model and move it to the current GPU
    model = model.to(device)
    # Wrap the model with DDP
    ddp_model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.Adam(ddp_model.parameters())
    criterion = hyperparameters.criterion

    scaler = GradScaler()  # FP16 scaler
    accumulation_steps = 4
    adj_mat = hyperparameters.adj_mat.to(device)


    # Training Loop
    for epoch in range(hyperparameters.epochs):
        model.train()  # Set model to training mode
        epoch_loss = 0.0
        running = torch.zeros((), device=device)
        # C = 0

        optimizer.zero_grad(set_to_none=True)


        for step, batch in enumerate(tqdm(hyperparameters.train_loader, desc=f"Epoch {epoch + 1}/{hyperparameters.epochs} - Training")):
            inputs, targets = batch
            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()

            # Forward pass
            b, t, n = inputs.shape
            inputs = inputs.reshape((b, t, n, 1))  # Add a channel dimension


            with autocast():
                outputs = ddp_model(inputs, adj_mat)
                outputs = outputs.transpose(1, 2)
                loss = criterion(outputs, targets)
                loss = loss / accumulation_steps

            
            scaler.scale(loss).backward()


            if (step + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)

            running += loss.detach() * accumulation_steps
        
        epoch_loss = reduce_mean(running / len(hyperparameters.train_loader))

            # epoch_loss += loss.item()
            # loss.backward()
            # optimizer.step()

        # avg_train_loss = epoch_loss / len(hyperparameters.train_loader)
        print(f"Epoch {epoch + 1}/{hyperparameters.epochs} - Training Loss: {epoch_loss:.4f}")

    # Clean up
    prefix_str = 'weather2k_gat_gru_traffic_pred'
    # Save the model
    torch.save(model.state_dict(), f'./torch_models/{prefix_str}_gat_gru_traffic_prediction.pth')


    dist.destroy_process_group()










def load_data(batch_size):

    data_path = os.path.join('weather2k_1866x40896x3.npz')
    fdata = np.load(data_path)
    raw_data = fdata['data']
    raw_data = raw_data.transpose(1, 0, 2)
    data = raw_data[:, :, -2]
    adj = np.load(os.path.join('weather2k_adj_dist.npz'))['adj_dist']
    adj = adj / 1000.0
    
    data, scaler = normalize_dataset(data, 'max01', False)
    

    data_train, data_val, data_test = split_data_by_ratio(data, 0.2, 0.2)

    lags = 12
    horizon = 12


    # add time window
    x_tra, y_tra = Add_Window_Horizon(data_train, window=lags, horizon=horizon, single=False)
    x_val, y_val = Add_Window_Horizon(data_val, window=lags, horizon=horizon, single=False)
    x_test, y_test = Add_Window_Horizon(data_test, window=lags, horizon=horizon, single=False)


    print('Train: ', x_tra.shape, y_tra.shape)
    print('Val: ', x_val.shape, y_val.shape)
    print('Test: ', x_test.shape, y_test.shape)


    adj, adj_weight = build_adj(adj)

    tra_loader = data_loader(x_tra, y_tra, batch_size, shuffle=True, drop_last=True)
    val_loader = data_loader(x_val, y_val, batch_size, shuffle=False, drop_last=True)
    tst_loader = data_loader(x_test, y_test, batch_size, shuffle=False, drop_last=False)

    return tra_loader, val_loader, tst_loader, adj


if __name__ == "__main__":

    world_size = torch.cuda.device_count() # or os.environ['WORLD_SIZE']
    local_rank = int(os.environ['LOCAL_RANK']) # torchrun sets these environment variables

    batch_size = 32
    tra_loader, val_loader, tst_loader, adj = load_data(batch_size)

    hyperparameters = Hyperparameters(tra_loader, adj)

# Model Initialization
    model = GAT_GRU_Forecaster(hyperparameters.n_feat, hyperparameters.n_hidden, hyperparameters.n_heads, hyperparameters.output_dim, hyperparameters.gru_hidden)
    optimizer = optim.Adam(model.parameters(), lr=hyperparameters.learning_rate, weight_decay=hyperparameters.weight_decay)

    
    train(local_rank, world_size, model, optimizer, hyperparameters) # no need for the spawn function


